
# 数据清洗以及特征提取

---

### 实验内容

本实验中，我们将学习回归分析算法 Lasso 算法。使用 Python scikit-learn 机器学习工具。

### 实验知识点

*   回归
*   嵌入式选择
*   正则化
*   Lasso 算法

---

### 实验步骤

### 下载数据

**☞ 示例代码：**


```python
!wget -nc http://labfile.oss.aliyuncs.com/courses/1010/data.csv
```

**☞ 动手练习：**


```python

```

### 回归预测

#### 理论基础

#### 回归

如果我们进行的是离散数据的预测，则称为 “分类”，而若预测的是连续数据，则称为 “回归”。线性回归算法包括线性回归、岭回归、套索回归、支持向量机回归、集成学习器回归算法等等。本实验使用套索回归（Lasso），在后面的学习中，我们会看到套索回归简单并且性状优良。

#### 特征选择以及正则化

特征选择同降维类似，保留重要特征，去除无关特征或者冗余特征，目的是减轻维数灾难，并减轻学习难度。方法非常的多，可以参考周志华老师的《机器学习》。我们后面会学习其中的嵌入式选择。

我们知道线性回归使用最小二乘法来选择最优参数。对于给定数据集 D，样例数为 m，每个样例特征数目为 n，当 n>m 时，难以求解，并且对于 n 大 m 小的情况，非常容易出现过拟合。这时候，我们可以引入正则化解决这个问题。
 
参考：

[最小二乘法](http://blog.51cto.com/sbp810050504/1269572)

[正则化（仅参考其中正则化方程形式以及 Lp 范数两部分知识）即可](https://zhuanlan.zhihu.com/p/29957294)

#### 嵌入式选择以及 Lasso 算法

已知数据集 D：{（X1,y1），...，（Xm,ym）}，在线性回归中，根据最小二乘法，最小化预测值与实际值 y 的误差平方：

$$ \min _{w} \sum_{i=1}^{m}\left(y_{i}-w^{T} x_{i}\right)^{2}$$

使用 L1 范数正则化，

$$ \min _{w} \sum_{i=1}^{m}\left(y_{i}-w^{T} x_{i}\right)^{2}+\lambda\|w\|_{1}$$

使用正则化过后，最小化过程倾向于得到较小的系数 w， 此时 X 的权重降低，学习器过拟合风险降低。

以二维为例（X 仅有两个特征，w 仅有两个分量 w1 和 w2），我们注意到，如果以 w1 和 w2 为坐标轴，平方误差项（方程第一项）的等值线（（w1，w2）空间中平方误差项取值相同的点的连线）实际是椭圆，而正则项（方程第二项）的等值线是直线，公式 2 的解是这两条等值线的交点。在二维平面上，椭圆和直线易相交于坐标轴上，使 w1 或者 w2 为零。也就是说，使用 L1 范数正则化，W 易得到较多的零分量。具体可以参考周志华老师《机器学习》第 11 章第 4 节。

这便是一种嵌套式选择。因为 w 具有零分量，意味着部分特征将会被舍弃。所以，嵌套式选择是在学习器的算法中包含了特征选择，也就是说学习器在训练的同时，也进行了特征选择。

这也是 Lasso 回归算法的原理，所以这个算法具有优化过拟合风险、进行特征选择的优势。

#### 回归预测

Lasso 算法具有特征选择的作用，非常适合数据特征多，特征高度相关的环境。波士顿房屋的数据征就具有这个特点。

接下来我们导入实验 1 处理好的数据，


```python
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Lasso

data = pd.read_csv('data.csv')

import warnings

def ignore_warn(*arfs, **kwargs):
    pass

warnings.warn = ignore_warn  # 忽略无意义的警告

l_train = len(data[data['SalePrice'].notnull()])  # 训练集长度
train = data[:l_train]  # 训练集
y = train['SalePrice']  # 预测目标
X = train.drop('SalePrice', axis=1).values  # 特征向量
```


```python

```

使用交叉验证法计算精度。不同于分类问题，回归问题，我们通过计算预测房价与实际房价的误差平方的平均值（偏差）来表征误差。参考 [Mean squared error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error)

Lasso 算法重要的参数是正则项系数，这里选择 0.0005 时误差较低。


```python
def scoring(model):  # 定义函数 scoring，利用5折交叉验证法计算测试误差
    r = cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=5)
    score = -r
    return(score)

clf = Lasso(alpha=0.0005)  # 参数设置
score = scoring(clf)  # 调用 scoring 函数计算回归偏差
print("偏差: {:.4f} ({:.4f})".format(score.mean(), score.std()))
# 交叉验证法偏差平均值及标准差
```


```python

```

sklearn 提供 coef_ 函数返回特征的权重系数。我们使用这个函数观察嵌入式选择后，还有多少特征保留下来。


```python
clf = Lasso(alpha=0.0005)  # 参数设置
clf.fit(X, y)  # 训练模型
print('特征总数：%d' % len(data.columns))
print('嵌入式选择后，保留特征数：%d' % np.sum(clf.coef_ != 0))  # 计算并显示嵌入式选择后，保留的特征数。
```


```python

```

在课程 1001 中介绍了学习曲线，这里我们也用学习曲线来观察学习器性能，最后可以看到我们的学习器很健康。


```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
%matplotlib inline

def plot_learning_curve(estimator, title, X, y, cv=10,
                        train_sizes=np.linspace(.1, 1.0, 5)):  # 定义 plot_learning_curve 函数绘制学习曲线
    plt.figure()
    plt.title(title)  # 图片标题
    plt.xlabel('Training examples')  # 横坐标
    plt.ylabel('Score')  # 纵坐标
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, scoring="neg_mean_squared_error",
                                                            train_sizes=train_sizes)  # 交叉验证法计算训练误差，测试误差
    train_scores_mean = np.mean(-train_scores, axis=1)  # 计算训练误差平均值
    train_scores_std = np.std(-train_scores, axis=1)  # 训练误差方差
    test_scores_mean = np.mean(-test_scores, axis=1)  # 测试误差平均值
    test_scores_std = np.std(-test_scores, axis=1)  # 测试误差方差
    plt.grid()  # 增加网格

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std,
                     alpha=0.1, color='g')  # 颜色填充
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std,
                     alpha=0.1, color='r')  # 颜色填充
    plt.plot(train_sizes, train_scores_mean, 'o-', color='g',
             label='traning score')  # 绘制训练误差曲线
    plt.plot(train_sizes, test_scores_mean, 'o-', color='r',
             label='testing score')  # 绘制测试误差曲线
    plt.legend(loc='best')
    return plt

clf = Lasso(alpha=0.0005)
g = plot_learning_curve(clf, 'Lasso', X, y)  # 调用 plot_learning_curve 绘制学习曲线
```


```python

```

![image](https://doc.shiyanlou.com/document-uid598017labid4392timestamp1515479672194.png)

上面这段代码在实验楼线上环境中运行时间较长，请耐心等待。最后进行预测并保存数据：


```python
clf = Lasso(alpha=0.0005)  # 参数设置
clf.fit(X, y)
test = data[l_train:].drop('SalePrice', axis=1).values  # 测试集数据
predict = np.exp(clf.predict(test))  # 预测
resul = pd.DataFrame()
resul['SalePrice'] = predict
resul.to_csv('submission.csv', index=False)  # 将结果写入 submission.csv
```


```python

```


```python
# 查看 submission.csv
pd.read_csv('submission.csv')
```


```python

```

### 实验总结

本实验学习了 Lasso 回归算法及相关的理论知识，我们看到 Lasso 具有减小过拟合风险、特征选择双重作用，适合于特征多，相关性高的数据集。

## 课后作业

提高预测精度有哪些途径呢？

<div style="color: #999;font-size: 12px;">©️ 本课程内容，由作者授权实验楼发布，未经允许，禁止转载、下载及非法传播。</div>
